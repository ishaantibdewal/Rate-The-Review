{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rate The Review\n",
    "\n",
    "**Name(s)**: Chris Chen, Ishaan Tibdewal\n",
    "\n",
    "**Website Link**: https://chrisc1124.github.io/Rate-The-Review/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from pathlib import Path\n",
    "\n",
    "import plotly.express as px\n",
    "pd.options.plotting.backend = 'plotly'\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report, mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "from dsc80_utils import *  # Feel free to uncomment and use this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('data')\n",
    "recipes = pd.read_csv(data_dir / 'RAW_recipes.csv')\n",
    "interactions = pd.read_csv(data_dir / 'RAW_interactions.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_interactions = recipes.merge(\n",
    "    interactions,\n",
    "    left_on='id',\n",
    "    right_on='recipe_id',\n",
    "    how='left',\n",
    ")\n",
    "recipes_interactions['rating'] = recipes_interactions['rating'].replace(0, np.nan)\n",
    "recipes_interactions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_rating = recipes_interactions.groupby('id')['rating'].mean()\n",
    "avg_rating.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_interactions = recipes_interactions.merge(\n",
    "    avg_rating.rename(\"avg_rating\"),\n",
    "    left_on=\"id\",\n",
    "    right_index=True,\n",
    "    how=\"left\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ratings of 0 in the raw interactions denote missing feedback. Replacing them with `NaN` prevents them from pulling recipe averages downward and keeps the mean focused on true ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Cleaning and Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_interactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_nutrition(nutr):\n",
    "    nutr = nutr.strip('[]')\n",
    "    vals = nutr.split(',')\n",
    "    return [float(val) for val in vals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_interactions['nutrition'] = recipes_interactions['nutrition'].apply(process_nutrition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding columns for each nutrition factor for special considerations\n",
    "nutrition_df = pd.DataFrame(recipes_interactions['nutrition'].tolist())\n",
    "nutrition_df.columns = ['calories(#)', 'total fat(pdv)', 'sugar(pdv)', 'sodium(pdv)', \n",
    "                  'protein(pdv)', 'saturated fat(pdv)', 'carbohydrates(pdv)'  ]\n",
    "recipes_interactions = recipes_interactions.join(nutrition_df)\n",
    "recipes_interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_interactions['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_interactions['minutes'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_interactions['minutes'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_interactions['rating'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_sodium = recipes_interactions.groupby(\"rating\")[\"total fat(pdv)\"].mean()\n",
    "avg_sodium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_sodium = recipes_interactions.groupby(\"rating\")[\"sodium(pdv)\"].mean()\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(avg_sodium.index, avg_sodium.values, marker='o', linewidth=2)\n",
    "\n",
    "plt.xlabel(\"Rating\")\n",
    "plt.ylabel(\"Average Sodium (PDV)\")\n",
    "plt.title(\"Average Sodium Content vs. Rating\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sodium_by_rating = (\n",
    "    recipes_interactions\n",
    "    .dropna(subset=[\"rating\"])\n",
    "    .assign(rating_int=lambda df: df[\"rating\"].astype(int))  # ratings are discrete\n",
    "    .groupby(\"rating_int\")[\"sodium(pdv)\"]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "fig = px.bar(\n",
    "    sodium_by_rating,\n",
    "    x=\"rating_int\",\n",
    "    y=\"sodium(pdv)\",\n",
    "    labels={\"rating_int\": \"Rating\", \"sodium_pdv\": \"Average sodium (% DV)\"},\n",
    "    title=\"Average sodium by rating\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carbs_by_rating = (\n",
    "    recipes_interactions\n",
    "    .dropna(subset=[\"rating\"])\n",
    "    .assign(rating_int=lambda df: df[\"rating\"].astype(int))  # ratings are discrete\n",
    "    .groupby(\"rating_int\")[\"carbohydrates(pdv)\"]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "fig = px.bar(\n",
    "    carbs_by_rating,\n",
    "    x=\"rating_int\",\n",
    "    y=\"carbohydrates(pdv)\",\n",
    "    title=\"Average carbs by rating\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sugar_by_rating = (\n",
    "    recipes_interactions\n",
    "    .dropna(subset=[\"rating\"])\n",
    "    .assign(rating_int=lambda df: df[\"rating\"].astype(int))  # ratings are discrete\n",
    "    .groupby(\"rating_int\")[\"sugar(pdv)\"]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "fig = px.bar(\n",
    "    sugar_by_rating,\n",
    "    x=\"rating_int\",\n",
    "    y=\"sugar(pdv)\",\n",
    "    title=\"Average sugar by rating\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_by_rating = (\n",
    "    recipes_interactions\n",
    "    .dropna(subset=[\"rating\"])\n",
    "    .assign(rating_int=lambda df: df[\"rating\"].astype(int))  # ratings are discrete\n",
    "    .groupby(\"rating_int\")[\"protein(pdv)\"]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "fig = px.bar(\n",
    "    protein_by_rating,\n",
    "    x=\"rating_int\",\n",
    "    y=\"protein(pdv)\",\n",
    "    title=\"Average protein by rating\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_interactions['total fat(pdv)'].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_interactions.iloc[56057]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_filtered = recipes_interactions[recipes_interactions['minutes'] <= 600]\n",
    "minutes_by_rating = (\n",
    "    recipes_interactions\n",
    "    .dropna(subset=[\"rating\"])\n",
    "    .assign(rating_int=lambda df: df[\"rating\"].astype(int))  # ratings are discrete\n",
    "    .groupby(\"rating_int\")[\"minutes\"]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "fig = px.bar(\n",
    "    minutes_by_rating,\n",
    "    x=\"rating_int\",\n",
    "    y=\"minutes\",\n",
    "    title=\"Average minutes by rating\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_fat = px.scatter(\n",
    "    recipes_interactions,\n",
    "    x=\"total fat(pdv)\",\n",
    "    y=\"calories(#)\",\n",
    "    title=\"Total Fat vs Calories\",\n",
    "    opacity=0.4,\n",
    "    range_x=[0, 500],\n",
    "    range_y=[0, 5000],\n",
    "    trendline='ols',\n",
    "    trendline_color_override='red'\n",
    ")\n",
    "fig_fat.show()\n",
    "print(np.corrcoef(recipes_interactions['total fat(pdv)'], recipes_interactions['calories(#)']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_prot = px.scatter(\n",
    "    recipes_interactions,\n",
    "    x=\"protein(pdv)\",\n",
    "    y=\"calories(#)\",\n",
    "    title=\"Protein vs Total Fat\",\n",
    "    opacity=0.4,\n",
    "    range_x=[0, 500],\n",
    "    range_y=[0, 5000],\n",
    "    trendline='ols',\n",
    "    trendline_color_override='red'\n",
    ")\n",
    "fig_prot.show()\n",
    "print(np.corrcoef(recipes_interactions['protein(pdv)'], recipes_interactions['calories(#)']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_sug = px.scatter(\n",
    "    recipes_interactions,\n",
    "    x=\"sugar(pdv)\",\n",
    "    y=\"calories(#)\",\n",
    "    title=\"Sugar vs Calories\",\n",
    "    opacity=0.4,\n",
    "    range_x=[0, 500],\n",
    "    range_y=[0, 5000],\n",
    "    trendline='ols',\n",
    "    trendline_color_override='red'\n",
    ")\n",
    "fig_sug.show()\n",
    "print(np.corrcoef(recipes_interactions['sugar(pdv)'], recipes_interactions['calories(#)']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_fat = px.scatter(\n",
    "    recipes_interactions,\n",
    "    x=\"carbohydrates(pdv)\",\n",
    "    y=\"calories(#)\",\n",
    "    title=\"Total Fat vs Calories\",\n",
    "    opacity=0.4,\n",
    "    range_x=[0, 500],\n",
    "    range_y=[0, 5000],\n",
    "    trendline='ols',\n",
    "    trendline_color_override='red'\n",
    ")\n",
    "fig_fat.show()\n",
    "print(np.corrcoef(recipes_interactions['carbohydrates(pdv)'], recipes_interactions['calories(#)']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_fat = px.scatter(\n",
    "    recipes_interactions,\n",
    "    x=\"protein(pdv)\",\n",
    "    y=\"calories(#)\",\n",
    "    title=\"Protein vs Calories\",\n",
    "    opacity=0.4,\n",
    "    range_x=[0, 500],\n",
    "    range_y=[0, 5000],\n",
    "    trendline='ols',\n",
    "    trendline_color_override='red'\n",
    ")\n",
    "fig_fat.show()\n",
    "print(np.corrcoef(recipes_interactions['protein(pdv)'], recipes_interactions['calories(#)']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_sug = px.scatter(\n",
    "    recipes_interactions,\n",
    "    x=\"sugar(pdv)\",\n",
    "    y=\"total fat(pdv)\",\n",
    "    title=\"Sugar vs fat\",\n",
    "    opacity=0.4,\n",
    "    range_x=[0, 500],\n",
    "    range_y=[0, 500],\n",
    "    trendline='ols',\n",
    "    trendline_color_override='red'\n",
    ")\n",
    "fig_sug.show()\n",
    "print(np.corrcoef(recipes_interactions['sugar(pdv)'], recipes_interactions['total fat(pdv)']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_sug = px.scatter(\n",
    "    recipes_interactions,\n",
    "    x=\"sodium(pdv)\",\n",
    "    y=\"total fat(pdv)\",\n",
    "    title=\"Sugar vs fat\",\n",
    "    opacity=0.4,\n",
    "    range_x=[0, 500],\n",
    "    range_y=[0, 500],\n",
    "    trendline='ols',\n",
    "    trendline_color_override='red'\n",
    ")\n",
    "fig_sug.show()\n",
    "print(np.corrcoef(recipes_interactions['sodium(pdv)'], recipes_interactions['total fat(pdv)']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_interactions['review_length'] = recipes_interactions['review'].str.len()\n",
    "recipes_interactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(recipes_interactions[['name', 'id', 'minutes', 'rating', 'calories(#)', 'review_length']]\n",
    "    .head()\n",
    "    .to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_interactions.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_lengths = recipes_interactions['review_length'].dropna()\n",
    "\n",
    "fig = px.histogram(\n",
    "    x=review_lengths,\n",
    "    nbins=100,\n",
    "    title='Distribution of Review Lengths (Characters)',\n",
    "    labels={'x': 'Review Length (characters)', 'count': 'Frequency'},\n",
    "    opacity=0.7\n",
    ")\n",
    "\n",
    "review_mean = review_lengths.mean()\n",
    "review_median = review_lengths.median()\n",
    "\n",
    "fig.add_vline(\n",
    "    x=review_mean,\n",
    "    line_dash=\"dash\",\n",
    "    line_color=\"red\",\n",
    "    line_width=2,\n",
    "    annotation_text=f\"Mean: {review_mean:.0f}\",\n",
    "    annotation_position=\"top right\"\n",
    ")\n",
    "\n",
    "fig.add_vline(\n",
    "    x=review_median,\n",
    "    line_dash=\"dash\",\n",
    "    line_color=\"blue\",\n",
    "    line_width=2,\n",
    "    annotation_text=f\"Median: {review_median:.0f}\",\n",
    "    annotation_position=\"top left\"\n",
    ")\n",
    "\n",
    "#filtered to 99th percentile for better visualization\n",
    "upper_bound = review_lengths.quantile(0.99)\n",
    "fig.update_xaxes(range=[0, upper_bound * 1.3])\n",
    "fig.show()\n",
    "fig.write_html('rate-the-review/assets/review_len_dist.html', include_plotlyjs='cdn')\n",
    "\n",
    "print(f\"Review length statistics:\")\n",
    "print(f\"  Mean: {review_mean:.2f} characters\")\n",
    "print(f\"  Median: {review_median:.2f} characters\")\n",
    "print(f\"  Min: {review_lengths.min():.0f} characters\")\n",
    "print(f\"  Max: {review_lengths.max():.0f} characters\")\n",
    "print(f\"  95th percentile: {upper_bound:.0f} characters\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bivariate analysis\n",
    "review_rating_data = recipes_interactions.dropna(subset=['review_length', 'rating']).copy()\n",
    "review_rating_data['rating_int'] = review_rating_data['rating'].astype(int)\n",
    "\n",
    "upper_bound = review_rating_data['review_length'].quantile(0.99)\n",
    "review_rating_data_filtered = review_rating_data[\n",
    "    review_rating_data['review_length'] <= upper_bound\n",
    "]\n",
    "\n",
    "fig = px.violin(\n",
    "    review_rating_data_filtered,\n",
    "    x='rating_int',\n",
    "    y='review_length',\n",
    "    title='Distribution of Review Lengths by Rating',\n",
    "    labels={'rating_int': 'Rating', 'review_length': 'Review Length (characters)'},\n",
    "    color='rating_int',\n",
    "    color_discrete_sequence=px.colors.qualitative.Set2,\n",
    "    box=True, \n",
    "    points=False  \n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    width=800,\n",
    "    height=500,\n",
    "    showlegend=False\n",
    ")\n",
    "fig.show()\n",
    "fig.write_html('rate-the-review/assets/review_length_by_rating_violin.html', include_plotlyjs='cdn')\n",
    "\n",
    "\n",
    "avg_review_length_by_rating = (\n",
    "    review_rating_data\n",
    "    .groupby('rating_int')['review_length']\n",
    "    .agg(['mean', 'median', 'count'])\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "fig_bar = px.bar(\n",
    "    avg_review_length_by_rating,\n",
    "    x='rating_int',\n",
    "    y='mean',\n",
    "    title='Average Review Length by Rating',\n",
    "    labels={'rating_int': 'Rating', 'mean': 'Average Review Length (characters)'},\n",
    "    text='mean',\n",
    "    color='rating_int',\n",
    "    color_discrete_sequence=px.colors.qualitative.Set2\n",
    ")\n",
    "fig_bar.update_traces(texttemplate='%{text:.0f}', textposition='outside')\n",
    "fig_bar.update_layout(width=700, height=500, showlegend=False)\n",
    "fig_bar.show()\n",
    "fig_bar.write_html('rate-the-review/assets/avg_review_length_by_rating.html', include_plotlyjs='cdn')\n",
    "\n",
    "print(\"\\nAverage review length by rating:\")\n",
    "print(avg_review_length_by_rating)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aggregate analysis\n",
    "review_agg_data = recipes_interactions.dropna(subset=['review_length', 'rating', 'n_steps', 'n_ingredients', 'calories(#)', 'minutes']).copy()\n",
    "\n",
    "#review length categories\n",
    "review_agg_data['review_category'] = pd.cut(\n",
    "    review_agg_data['review_length'],\n",
    "    bins=[0, 100, 250, 500, float('inf')],\n",
    "    labels=['Short', 'Medium', 'Long', 'Very Long']\n",
    ")\n",
    "\n",
    "review_aggregate = review_agg_data.groupby('review_category').agg({\n",
    "    'rating': ['mean', 'count'],\n",
    "    'n_steps': 'mean',\n",
    "    'n_ingredients': 'mean',\n",
    "    'calories(#)': 'mean',\n",
    "    'minutes': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "review_aggregate.columns = ['Average Rating', 'Count', 'Avg Steps', 'Avg Ingredients', 'Avg Calories', 'Avg Cooking Time (min)']\n",
    "review_aggregate = review_aggregate.reset_index()\n",
    "\n",
    "print(\"Average Rating, Steps, Ingredients, Calories, and Cooking Time by Review Length:\")\n",
    "print(review_aggregate.to_markdown(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_rating_analysis = recipes_interactions.dropna(subset=['review_length', 'rating']).copy()\n",
    "review_rating_analysis['rating_int'] = review_rating_analysis['rating'].astype(int)\n",
    "\n",
    "rating_grouped_stats = review_rating_analysis.groupby('rating_int')['review_length'].agg([\n",
    "    'count', 'mean', 'median', 'min', 'max'\n",
    "]).round(2)\n",
    "rating_grouped_stats.columns = ['Count', 'Mean Review Length', 'Median Review Length', 'Min Review Length', 'Max Review Length']\n",
    "rating_grouped_stats = rating_grouped_stats.reset_index()\n",
    "\n",
    "print(\"Review Length Statistics by Rating:\")\n",
    "rating_grouped_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Assessment of Missingness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMAR in our Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_interactions[recipes_interactions['rating'].isna()][['rating', 'review']].iloc[11].loc['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_interactions.isna().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_interactions['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_interactions[recipes_interactions['description'].isna()]['minutes'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_interactions['n_steps'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_interactions['n_ingredients'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_interactions[recipes_interactions['description'].isna()]['n_steps'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_interactions[recipes_interactions['description'].isna()]['n_ingredients'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We believe that the missingness of the `description` column in our recipe + interactions dataframe is most likely NMAR due to the fact that only authors who actaully really care about the recipe and making the recipe look better on the site will leave a description. Those who don't care as much about their recipe, or if it's a simple and straightforward recipe, may decide that having a description is not a priority and will leave it out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAR Dependency test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In our missingness dependency test, we want to examine the missingness of the `description` column by testing the relationship/dependency of its missingness with certain columns in our dataframe. Specifically, we are going to be looking at whether the missingess of `description` is dependent on the columns `n_steps`, which is the number of steps in the recipe, and `n_ingredients` which is the number of ingredients in the recipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_info = recipes_interactions.groupby('id').first()\n",
    "recipe_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MAR test for missingness of `description` dependent on `n_steps`\n",
    "\n",
    "- Null Hypothesis: Missingness of description does not depend on n_steps\n",
    "- Alternate Hypothesis: Missingness of description does depend on n_step\n",
    "- Test Statistic: difference of mean n_steps with missing desc and mean n_steps without missing desc\n",
    "- Significance Level: 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.662099Z",
     "start_time": "2019-10-31T23:36:28.660016Z"
    }
   },
   "outputs": [],
   "source": [
    "mask_desc = recipe_info[\"description\"].isna()\n",
    "df = recipe_info.copy()\n",
    "\n",
    "def diff_median_steps(mask):\n",
    "    g = df.groupby(mask)[\"n_steps\"].mean()\n",
    "    return g.loc[True] - g.loc[False]\n",
    "\n",
    "T_obs = diff_median_steps(mask_desc)\n",
    "\n",
    "stats = []\n",
    "for _ in range(2000):\n",
    "    shuffled = np.random.permutation(mask_desc.to_numpy())\n",
    "    stats.append(diff_median_steps(shuffled))\n",
    "\n",
    "p_val = (stats >= T_obs).mean()\n",
    "print(f'T_obs: {T_obs}, p_val: {p_val}')\n",
    "print('we fail to reject the null hypothesis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = recipe_info.copy()\n",
    "df[\"missing_desc\"] = df[\"description\"].isna().map({True: \"Missing\", False: \"Present\"})\n",
    "\n",
    "curves = [\n",
    "    df.loc[df[\"missing_desc\"] == \"Missing\", \"n_steps\"].dropna(),\n",
    "    df.loc[df[\"missing_desc\"] == \"Present\", \"n_steps\"].dropna(),\n",
    "]\n",
    "\n",
    "fig_kde = ff.create_distplot(\n",
    "    curves,\n",
    "    group_labels=[\"Missing\", \"Present\"],\n",
    "    show_hist=False,\n",
    "    show_rug=False,\n",
    "    curve_type=\"kde\",\n",
    ")\n",
    "fig_kde.update_layout(\n",
    "    title=\"n_steps by Description Missingness\",\n",
    "    xaxis_title=\"# Steps\",\n",
    "    yaxis_title=\"density\",\n",
    ")\n",
    "fig_kde.show()\n",
    "fig_kde.write_html('rate-the-review/assets/n_steps_missingness_kde.html', include_plotlyjs='cdn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_perm = px.histogram(\n",
    "    stats,\n",
    "    nbins=40,\n",
    "    histnorm=\"probability\",\n",
    "    title=\"Null Distribution of mean n_steps difference (Missing - Present)\",\n",
    "    labels={\"value\": \" Mean Difference\", \"count\": \"probability\"},\n",
    ")\n",
    "fig_perm.add_vline(x=abs(T_obs), line_color=\"red\", line_width=2, annotation_text=\"observed\", annotation_position=\"top right\")\n",
    "fig_perm.show()\n",
    "fig_perm.write_html('rate-the-review/assets/n_steps_permutation_test.html', include_plotlyjs='cdn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MAR test for missingness of `description` dependent on `n_ingredients`\n",
    "- Null Hypothesis: missingness of description does not depend on n_ingredients\n",
    "- Alternate Hypothesis: missingness of description does depend on n_ingredients\n",
    "- Test Statistic: absolute difference of mean n_ingredients (missing desc) and mean n_ingredients (not missing desc)\n",
    "- Significance Level: 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_desc = recipe_info[\"description\"].isna()\n",
    "\n",
    "def diff_mean_ingredients(mask):\n",
    "    g = recipe_info.groupby(mask)[\"n_ingredients\"].mean()\n",
    "    return np.abs(g.loc[True] - g.loc[False])\n",
    "\n",
    "T_obs_ing = diff_mean_ingredients(mask_desc)\n",
    "\n",
    "stats_ing = []\n",
    "for _ in range(2000):\n",
    "    shuffled = np.random.permutation(mask_desc.to_numpy())\n",
    "    stats_ing.append(diff_mean_ingredients(shuffled))\n",
    "stats_ing = np.array(stats_ing)\n",
    "p_val_ing = (stats_ing >= T_obs_ing).mean()\n",
    "print(f'T_obs: {T_obs_ing}, p_val: {p_val_ing}')\n",
    "print('we reject the null hypothesis')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curves = [\n",
    "    recipe_info.loc[mask_desc, \"n_ingredients\"].dropna(),\n",
    "    recipe_info.loc[~mask_desc, \"n_ingredients\"].dropna(),\n",
    "]\n",
    "\n",
    "fig_kde_ing = ff.create_distplot(\n",
    "    curves,\n",
    "    group_labels=[\"Missing\", \"Present\"],\n",
    "    show_hist=False,\n",
    "    show_rug=False,\n",
    "    curve_type=\"kde\",\n",
    ")\n",
    "fig_kde_ing.update_layout(\n",
    "    title=\"n_ingredients by Description Missingness\",\n",
    "    xaxis_title=\"# Ingredients\",\n",
    "    yaxis_title=\"density\",\n",
    ")\n",
    "fig_kde_ing.show()\n",
    "fig_kde_ing.write_html('rate-the-review/assets/n_ingredients_missingness_kde.html', include_plotlyjs='cdn')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_perm_ing = px.histogram(\n",
    "    stats_ing,\n",
    "    nbins=40,\n",
    "    histnorm=\"probability\",\n",
    "    title=\"Distribution of |mean difference| of n_ingredients (missing - present)\",\n",
    "    labels={\"value\": \"Absolute Median Difference\", \"count\": \"probability\"},\n",
    ")\n",
    "fig_perm_ing.add_vline(\n",
    "    x=T_obs_ing,\n",
    "    line_color=\"red\",\n",
    "    line_width=2,\n",
    "    annotation_text=\"observed\",\n",
    "    annotation_position=\"top right\",\n",
    ")\n",
    "fig_perm_ing.show()\n",
    "fig_perm_ing.write_html('rate-the-review/assets/n_ingredients_permutation_test.html', include_plotlyjs='cdn')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Hypothesis Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_interactions.head()[['review']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Null Hypothesis: Long and short reviews recieve the same ratings on average\n",
    "- Alternate Hypothesis: Long reviews recieve lower ratings on average compared to short reviews\n",
    "- Test Statistic: Difference in means of long reviews and short reviews\n",
    "- Significance Level: 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "recipes_interactions['review_length'] = recipes_interactions['review'].str.len()\n",
    "review_analysis = recipes_interactions.dropna(subset=['review_length', 'rating']).copy()\n",
    "\n",
    "#classifying reviews as long or short\n",
    "review_length_median = review_analysis['review_length'].median()\n",
    "review_analysis['is_long_review'] = review_analysis['review_length'] > review_length_median\n",
    "review_analysis['review_category'] = review_analysis['is_long_review'].map({True: 'Long Review', False: 'Short Review'})\n",
    "\n",
    "#observed \n",
    "long_review_ratings = review_analysis[review_analysis['is_long_review'] == True]['rating']\n",
    "short_review_ratings = review_analysis[review_analysis['is_long_review'] == False]['rating']\n",
    "observed_diff = long_review_ratings.mean() - short_review_ratings.mean()\n",
    "\n",
    "n_repetitions = 10000\n",
    "np.random.seed(42)\n",
    "ratings = review_analysis['rating'].values\n",
    "differences = []\n",
    "\n",
    "for _ in range(n_repetitions):\n",
    "    shuffled_labels = np.random.permutation(review_analysis['is_long_review'])\n",
    "    long_mean = ratings[shuffled_labels].mean()\n",
    "    short_mean = ratings[~shuffled_labels].mean()\n",
    "    diff = long_mean - short_mean\n",
    "    differences.append(diff)\n",
    "\n",
    "differences = np.array(differences)\n",
    "p_value = np.mean(differences <= observed_diff)\n",
    "\n",
    "# permutation test visualization\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(\n",
    "    x=differences,\n",
    "    nbinsx=50,\n",
    "    marker_color='mediumpurple',\n",
    "    opacity=0.7,\n",
    "    histnorm='probability density'\n",
    "))\n",
    "fig.add_vline(x=observed_diff, line_dash=\"dash\", line_color=\"red\", line_width=3,\n",
    "              annotation_text=f\"Observed: {observed_diff:.4f}\", annotation_position=\"top right\")\n",
    "fig.add_vline(x=0, line_dash=\"dot\", line_color=\"black\", line_width=1)\n",
    "fig.update_layout(\n",
    "    title='Distribution of mean difference in ratings for Long Review vs Short Review',\n",
    "    xaxis_title='Difference in Means (Long - Short)',\n",
    "    yaxis_title='Density',\n",
    "    width=800,\n",
    "    height=500,\n",
    "    showlegend=False\n",
    ")\n",
    "fig.show()\n",
    "fig.write_html('rate-the-review/assets/hypothesis_test.html', include_plotlyjs='cdn')\n",
    "\n",
    "print(f\"P-value: {p_value:.4f}\")\n",
    "print('we reject the null hypothesis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Framing a Prediction Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our prediction problem is to identify whether an individual user's **review will be highly rated** (rating >= 4) or **low rated** (rating < 4) based on the review text they wrote and recipe characteristics. This framing helps platforms understand how review content relates to user satisfaction and can assist in identifying patterns in how users express their experiences.\n",
    "\n",
    "- **Target (response)**: `highly_rated`, where we label each individual review as 1 if the user's rating is at least 4.0 and 0 otherwise.\n",
    "- **Prediction type**: Binary classification.\n",
    "- **Features considered**: \n",
    "  - Primary feature: Review text (`review`) converted to TF-IDF vectors - captures what the user wrote in their review\n",
    "  - Secondary feature: Recipe metadata (`n_steps`) - number of steps in the recipe\n",
    "- **Unit of analysis**: Each row corresponds to an individual review (one user's rating and review text for a recipe). We only keep reviews that have both review text and a rating.\n",
    "- **Practical value**: Understanding the relationship between review text content and ratings can help platforms identify helpful review patterns, improve recommendation systems, and better understand user sentiment expression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#filter to reviews that have both review text and rating\n",
    "model_data = recipes_interactions.dropna(subset=['review', 'rating']).copy()\n",
    "\n",
    "#create binary target: 1 if individual rating >= 4, else 0\n",
    "model_data['highly_rated'] = (model_data['rating'] >= 4.0).astype(int)\n",
    "\n",
    "print(f\"Total reviews with both review text and rating: {len(model_data):,}\")\n",
    "print(f\"Unique recipes: {model_data['id'].nunique():,}\")\n",
    "print(\"\\nClass distribution (1 = highly rated review, rating >= 4):\")\n",
    "print(model_data['highly_rated'].value_counts())\n",
    "print(\"\\nClass proportions:\")\n",
    "print(model_data['highly_rated'].value_counts(normalize=True))\n",
    "print(\"\\nRating distribution:\")\n",
    "print(model_data['rating'].value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(\n",
    "    x=model_data['highly_rated'].value_counts().index.astype(str),\n",
    "    y=model_data['highly_rated'].value_counts().values,\n",
    "    labels={'x': 'Highly Rated (1=Yes, 0=No)', 'y': 'Count'},\n",
    "    title='Distribution of Target Variable'\n",
    ")\n",
    "fig.show()\n",
    "fig.write_html('rate-the-review/assets/target_distribution.html', include_plotlyjs='cdn')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a stratified train/test split and save the indices for reuse in later steps\n",
    "X = model_data.drop(columns=['highly_rated', 'avg_rating', 'rating'])  # Drop target and rating (keep only features)\n",
    "y = model_data['highly_rated']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y,  # Maintain class distribution across splits\n",
    ")\n",
    "\n",
    "train_indices = X_train.index\n",
    "test_indices = X_test.index\n",
    "\n",
    "print(f\"Training set size: {len(X_train):,} reviews\")\n",
    "print(f\"Test set size: {len(X_test):,} reviews\")\n",
    "print(f\"Unique recipes in training: {X_train['id'].nunique():,}\")\n",
    "print(f\"Unique recipes in test: {X_test['id'].nunique():,}\")\n",
    "print(\"\\nTraining set class distribution:\")\n",
    "print(y_train.value_counts())\n",
    "print(\"\\nTest set class distribution:\")\n",
    "print(y_test.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5 summary**\n",
    "\n",
    "- Reviews missing either `review` text or `rating` are removed (only a small fraction).\n",
    "- The binary target `highly_rated` equals 1 when the individual user's `rating >= 4.0` and 0 otherwise.\n",
    "- A stratified 80/20 train/test split is created and the indices are saved for reuse in Steps 6 and 7 so the evaluation remains consistent.\n",
    "- The resulting dataset is class-imbalanced (roughly 94% of reviews are highly rated), so later models will use techniques such as `class_weight='balanced'` to compensate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.662099Z",
     "start_time": "2019-10-31T23:36:28.660016Z"
    }
   },
   "outputs": [],
   "source": [
    "# Re-create the train/test split using the saved indices so every model uses identical data\n",
    "X_train = model_data.loc[train_indices].drop(columns=['highly_rated', 'avg_rating'])\n",
    "X_test = model_data.loc[test_indices].drop(columns=['highly_rated', 'avg_rating'])\n",
    "y_train = model_data.loc[train_indices, 'highly_rated']\n",
    "y_test = model_data.loc[test_indices, 'highly_rated']\n",
    "\n",
    "print(f\"Training samples: {len(X_train):,} reviews\")\n",
    "print(f\"Test samples: {len(X_test):,} reviews\")\n",
    "print(\"Features:\")\n",
    "print(\" - review (text, TF-IDF) - what the user wrote in their review\")\n",
    "print(\" - n_steps (numeric) - number of steps in the recipe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        ('text', TfidfVectorizer(stop_words='english'), 'review'),  # Review text as primary feature (no max_features limit)\n",
    "        ('numeric', 'passthrough', ['n_steps'])  # n_steps as secondary feature\n",
    "    ]\n",
    ")\n",
    "\n",
    "baseline_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', baseline_preprocessor),\n",
    "        ('classifier', LogisticRegression(random_state=42, class_weight='balanced', max_iter=1000))\n",
    "    ]\n",
    ")\n",
    "\n",
    "baseline_pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training baseline model...\")\n",
    "baseline_pipeline.fit(X_train, y_train)\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = baseline_pipeline.predict(X_train)\n",
    "y_test_pred = baseline_pipeline.predict(X_test)\n",
    "\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "train_f1_weighted = f1_score(y_train, y_train_pred, average='weighted')\n",
    "test_f1_weighted = f1_score(y_test, y_test_pred, average='weighted')\n",
    "train_f1_macro = f1_score(y_train, y_train_pred, average='macro')\n",
    "test_f1_macro = f1_score(y_test, y_test_pred, average='macro')\n",
    "\n",
    "# Calculate RMSE (Root Mean Squared Error)\n",
    "train_rmse = sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "test_rmse = sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "\n",
    "print(\"Baseline Model Performance\")\n",
    "print(f\"Train accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"\\nTrain F1 (weighted): {train_f1_weighted:.4f}\")\n",
    "print(f\"Test F1 (weighted): {test_f1_weighted:.4f}\")\n",
    "print(f\"Train F1 (macro): {train_f1_macro:.4f}\")\n",
    "print(f\"Test F1 (macro): {test_f1_macro:.4f}\")\n",
    "print(f\"\\nTrain RMSE: {train_rmse:.4f}\")\n",
    "print(f\"Test RMSE: {test_rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_test = confusion_matrix(y_test, y_test_pred)\n",
    "print(\"Test confusion matrix:\\n\", cm_test)\n",
    "print(classification_report(y_test, y_test_pred, target_names=['Low-rated (0)', 'Highly-rated (1)']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(\n",
    "    data=go.Heatmap(\n",
    "        z=cm_test,\n",
    "        x=['Predicted low-rated', 'Predicted highly-rated'],\n",
    "        y=['Actual low-rated', 'Actual highly-rated'],\n",
    "        colorscale='Blues',\n",
    "        text=cm_test,\n",
    "        texttemplate='%{text}',\n",
    "        colorbar=dict(title='Count')\n",
    "    )\n",
    ")\n",
    "fig.update_layout(title='Baseline Model Confusion Matrix (Test Set)', width=600, height=400)\n",
    "fig.show()\n",
    "fig.write_html('rate-the-review/assets/baseline_confusion_matrix.html', include_plotlyjs='cdn')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Baseline model recap**\n",
    "\n",
    "- **Features**: \n",
    "  - TF-IDF representations of `review` text (no max_features limit - uses all words) - what the user wrote in their review\n",
    "  - Numeric `n_steps` column passed through unchanged - number of steps in the recipe\n",
    "- **Target**: Binary classification - `highly_rated = 1` if user's rating >= 4, else 0\n",
    "- **Estimator**: Logistic regression with `class_weight='balanced'` and `max_iter=1000` to cope with class imbalance.\n",
    "- **Pipeline**: Uses a single `ColumnTransformer` + `Pipeline` so preprocessing and modeling are bundled together.\n",
    "- **Performance**: This provides a concrete baseline for the final model to beat.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.662099Z",
     "start_time": "2019-10-31T23:36:28.660016Z"
    }
   },
   "outputs": [],
   "source": [
    "# Re-create the train/test split using the saved indices so every model uses identical data\n",
    "X_train = model_data.loc[train_indices].drop(columns=['highly_rated', 'avg_rating'])\n",
    "X_test = model_data.loc[test_indices].drop(columns=['highly_rated', 'avg_rating'])\n",
    "y_train = model_data.loc[train_indices, 'highly_rated']\n",
    "y_test = model_data.loc[test_indices, 'highly_rated']\n",
    "\n",
    "print(f\"Training samples: {len(X_train):,} reviews\")\n",
    "print(f\"Test samples: {len(X_test):,} reviews\")\n",
    "print(\"\\nFinal Model Features:\")\n",
    "print(\" - review (text, TF-IDF) - what the user wrote in their review\")\n",
    "print(\" - n_steps (numeric, passthrough) - number of steps in the recipe (from baseline)\")\n",
    "print(\" - calories(#) (numeric, StandardScaler) - NEW: calories scaled\")\n",
    "print(\" - minutes (numeric, QuantileTransformer) - NEW: cooking time transformed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters to Tune\n",
    "\n",
    "Before tuning, we document which hyperparameters we plan to tune and why:\n",
    "\n",
    "1. **LogisticRegression C (regularization strength)**\n",
    "   - Why: Controls the trade-off between fitting the training data and preventing overfitting.\n",
    "     - Smaller C (e.g., 0.01, 0.1) = stronger regularization = simpler model, less overfitting\n",
    "     - Larger C (e.g., 10, 100) = weaker regularization = more complex model, risk of overfitting\n",
    "2. **LogisticRegression penalty (regularization type)**\n",
    "   - Why: Different regularization types (L1 vs L2) can lead to different model behaviors.\n",
    "     - L1 (Lasso): Creates sparsity by zeroing out coefficients, effectively doing feature selection.\n",
    "       Useful when many features might be irrelevant.\n",
    "     - L2 (Ridge): Shrinks coefficients but keeps all features. Generally more stable and \n",
    "       standard for text classification.\n",
    "   - Options: ['l1', 'l2'] - we'll test both to see which works better for our text classification task\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "# Final model preprocessor with new features\n",
    "final_preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        ('text', TfidfVectorizer(stop_words='english'), 'review'),\n",
    "        ('numeric_passthrough', 'passthrough', ['n_steps']),  # Keep baseline feature\n",
    "        ('calories_scaled', StandardScaler(), ['calories(#)']),  # New feature 1\n",
    "        ('minutes_quantile', QuantileTransformer(output_distribution='normal', random_state=42), ['minutes'])  # New feature 2\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Final model pipeline\n",
    "final_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', final_preprocessor),\n",
    "        ('classifier', LogisticRegression(random_state=42, class_weight='balanced', max_iter=1000))\n",
    "    ]\n",
    ")\n",
    "\n",
    "final_pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_l2 = {\n",
    "    'classifier__C': [1, 2, 3, 4, 5, 6],\n",
    "    'classifier__penalty': ['l2'],\n",
    "    'classifier__solver': ['lbfgs'],  #for L2\n",
    "    'classifier__max_iter': [1500, 1600, 1700]  \n",
    "}\n",
    "\n",
    "param_grid_l1 = {\n",
    "    'classifier__C': [1, 2, 3, 4, 5, 6],\n",
    "    'classifier__penalty': ['l1'],\n",
    "    'classifier__solver': ['liblinear'],  # for L1\n",
    "    'classifier__max_iter': [1500, 1600, 1700]  \n",
    "}\n",
    "\n",
    "# Run L2 first \n",
    "grid_search_l2 = GridSearchCV(final_pipeline, param_grid_l2, cv=5, scoring='f1_macro', n_jobs=-1, verbose=1)\n",
    "grid_search_l2.fit(X_train, y_train)\n",
    "\n",
    "# L1\n",
    "grid_search_l1 = GridSearchCV(final_pipeline, param_grid_l1, cv=5, scoring='f1_macro', n_jobs=-1, verbose=1)\n",
    "grid_search_l1.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare L2 and L1 results\n",
    "best_l2_score = grid_search_l2.best_score_\n",
    "best_l1_score = grid_search_l1.best_score_\n",
    "\n",
    "print(\"Grid Search Results Comparison\")\n",
    "print(f\"Best L2 cross-validation F1-macro: {best_l2_score:.4f}\")\n",
    "print(f\"Best L1 cross-validation F1-macro: {best_l1_score:.4f}\")\n",
    "print()\n",
    "\n",
    "if best_l2_score > best_l1_score:\n",
    "    best_model = grid_search_l2.best_estimator_\n",
    "    best_grid = grid_search_l2\n",
    "    print(f\"L2 model performed better\")\n",
    "else:\n",
    "    best_model = grid_search_l1.best_estimator_\n",
    "    best_grid = grid_search_l1\n",
    "    print(f\"L1 model performed better\")\n",
    "\n",
    "print(\"Best Hyperparameters:\")\n",
    "for param, value in best_grid.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "print(f\"\\nBest cross-validation F1-macro score: {best_grid.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate best model on test set\n",
    "y_train_pred = best_model.predict(X_train)\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "train_f1_weighted = f1_score(y_train, y_train_pred, average='weighted')\n",
    "test_f1_weighted = f1_score(y_test, y_test_pred, average='weighted')\n",
    "train_f1_macro = f1_score(y_train, y_train_pred, average='macro')\n",
    "test_f1_macro = f1_score(y_test, y_test_pred, average='macro')\n",
    "\n",
    "train_rmse = sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "test_rmse = sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "\n",
    "print(\"Final Model Performance (Best Hyperparameters)\")\n",
    "print(f\"Train accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"\\nTrain F1 (weighted): {train_f1_weighted:.4f}\")\n",
    "print(f\"Test F1 (weighted): {test_f1_weighted:.4f}\")\n",
    "print(f\"Train F1 (macro): {train_f1_macro:.4f}\")\n",
    "print(f\"Test F1 (macro): {test_f1_macro:.4f}\")\n",
    "print(f\"\\nTrain RMSE: {train_rmse:.4f}\")\n",
    "print(f\"Test RMSE: {test_rmse:.4f}\")\n",
    "\n",
    "# Compare to baseline\n",
    "print(\"Comparison to Baseline:\")\n",
    "print(f\"Baseline test accuracy: 0.9039\")\n",
    "print(f\"Final test accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Improvement: {test_accuracy - 0.9039:+.4f}\")\n",
    "print(f\"\\nBaseline test F1 (macro): 0.7216\")\n",
    "print(f\"Final test F1 (macro): {test_f1_macro:.4f}\")\n",
    "print(f\"Improvement: {test_f1_macro - 0.7216:+.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for final model\n",
    "cm_test_final = confusion_matrix(y_test, y_test_pred)\n",
    "print(\"Final Model Test Confusion Matrix:\")\n",
    "print(cm_test_final)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred, target_names=['Low-rated (0)', 'Highly-rated (1)']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrix\n",
    "fig = go.Figure(\n",
    "    data=go.Heatmap(\n",
    "        z=cm_test_final,\n",
    "        x=['Predicted low-rated', 'Predicted highly-rated'],\n",
    "        y=['Actual low-rated', 'Actual highly-rated'],\n",
    "        colorscale='Blues',\n",
    "        text=cm_test_final,\n",
    "        texttemplate='%{text}',\n",
    "        colorbar=dict(title='Count')\n",
    "    )\n",
    ")\n",
    "fig.update_layout(title='Final Model Confusion Matrix (Test Set)', width=600, height=400)\n",
    "fig.show()\n",
    "fig.write_html('rate-the-review/assets/final_model_confusion_matrix.html', include_plotlyjs='cdn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Fairness Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.666489Z",
     "start_time": "2019-10-31T23:36:28.664381Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fairness Analysis: Does the model perform differently for quick recipes vs slow recipes?\n",
    "# Evaluation metric: Precision for the positive class (highly_rated = 1)\n",
    "\n",
    "X_test = model_data.loc[test_indices].drop(columns=['highly_rated', 'avg_rating'])\n",
    "y_test = model_data.loc[test_indices, 'highly_rated']\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "test_data_with_minutes = model_data.loc[test_indices].copy()\n",
    "test_data_with_minutes['predicted'] = y_test_pred\n",
    "\n",
    "#splitting the data into quick and slow recipes\n",
    "median_minutes = test_data_with_minutes['minutes'].median()\n",
    "print(f\"Median cooking time: {median_minutes:.1f} minutes\")\n",
    "print(f\"Quick recipes ( {median_minutes:.1f} min): {(test_data_with_minutes['minutes'] <= median_minutes).sum():,} reviews\")\n",
    "print(f\"Slow recipes (> {median_minutes:.1f} min): {(test_data_with_minutes['minutes'] > median_minutes).sum():,} reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary group labels\n",
    "test_data_with_minutes['is_quick'] = test_data_with_minutes['minutes'] <= median_minutes\n",
    "\n",
    "# Calculate precision for each group\n",
    "# Precision = TP / (TP + FP) for the positive class (highly_rated = 1)\n",
    "\n",
    "def calculate_precision(y_true, y_pred, positive_class=1):\n",
    "    \"\"\"Calculate precision for the positive class.\"\"\"\n",
    "    tp = ((y_true == positive_class) & (y_pred == positive_class)).sum()\n",
    "    fp = ((y_true != positive_class) & (y_pred == positive_class)).sum()\n",
    "    if tp + fp == 0:\n",
    "        return 0.0\n",
    "    return tp / (tp + fp)\n",
    "\n",
    "# Quick recipes group\n",
    "quick_mask = test_data_with_minutes['is_quick']\n",
    "quick_precision = calculate_precision(\n",
    "    test_data_with_minutes.loc[quick_mask, 'highly_rated'],\n",
    "    test_data_with_minutes.loc[quick_mask, 'predicted']\n",
    ")\n",
    "\n",
    "# Slow recipes group\n",
    "slow_mask = ~test_data_with_minutes['is_quick']\n",
    "slow_precision = calculate_precision(\n",
    "    test_data_with_minutes.loc[slow_mask, 'highly_rated'],\n",
    "    test_data_with_minutes.loc[slow_mask, 'predicted']\n",
    ")\n",
    "\n",
    "observed_diff = quick_precision - slow_precision\n",
    "\n",
    "print(\"Observed Precision by Group\")\n",
    "print(f\"Quick recipes ( {median_minutes:.1f} min): {quick_precision:.4f}\")\n",
    "print(f\"Slow recipes (> {median_minutes:.1f} min): {slow_precision:.4f}\")\n",
    "print(f\"\\nDifference (Quick - Slow): {observed_diff:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypotheses\n",
    "\n",
    "- **Null Hypothesis**: Our model is fair. Its precision for quick recipes and slow recipes are roughly the same, and any differences are due to random chance.\n",
    "\n",
    "- **Alternative Hypothesis**: Our model is unfair. Its precision for quick recipes is different from its precision for slow recipes (two-sided test).\n",
    "\n",
    "### Permutation Test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "n_repetitions = 10000\n",
    "\n",
    "# Store the actual group labels and predictions/true values\n",
    "group_labels = test_data_with_minutes['is_quick'].values\n",
    "y_true_values = test_data_with_minutes['highly_rated'].values\n",
    "y_pred_values = test_data_with_minutes['predicted'].values\n",
    "\n",
    "def precision_diff_permuted(shuffled_groups):\n",
    "    quick_mask = shuffled_groups\n",
    "    slow_mask = ~shuffled_groups\n",
    "    \n",
    "    quick_prec = calculate_precision(\n",
    "        y_true_values[quick_mask],\n",
    "        y_pred_values[quick_mask]\n",
    "    )\n",
    "    slow_prec = calculate_precision(\n",
    "        y_true_values[slow_mask],\n",
    "        y_pred_values[slow_mask]\n",
    "    )\n",
    "    return quick_prec - slow_prec\n",
    "\n",
    "permuted_diffs = []\n",
    "for _ in range(n_repetitions):\n",
    "    shuffled = np.random.permutation(group_labels)\n",
    "    permuted_diffs.append(precision_diff_permuted(shuffled))\n",
    "\n",
    "permuted_diffs = np.array(permuted_diffs)\n",
    "\n",
    "p_value = np.mean(np.abs(permuted_diffs) >= np.abs(observed_diff))\n",
    "\n",
    "print(\"Permutation Test Results\")\n",
    "print(f\"Observed difference: {observed_diff:.4f}\")\n",
    "print(f\"P-value (two-sided): {p_value:.4f}\")\n",
    "print(f\"Significance level: 0.05\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the permutation test results\n",
    "fig = px.histogram(\n",
    "    x=permuted_diffs,\n",
    "    nbins=50,\n",
    "    histnorm='probability density',\n",
    "    title='Null Distribution of Precision Difference (Quick - Slow)',\n",
    "    labels={'x': 'Precision Difference (Quick - Slow)', 'y': 'Density'},\n",
    "    opacity=0.7\n",
    ")\n",
    "\n",
    "# Add observed value\n",
    "fig.add_vline(\n",
    "    x=observed_diff,\n",
    "    line_dash=\"dash\",\n",
    "    line_color=\"red\",\n",
    "    line_width=3,\n",
    "    annotation_text=f\"Observed: {observed_diff:.4f}\",\n",
    "    annotation_position=\"top right\"\n",
    ")\n",
    "\n",
    "# Add negative of observed value for two-sided test\n",
    "fig.add_vline(\n",
    "    x=-observed_diff,\n",
    "    line_dash=\"dash\",\n",
    "    line_color=\"red\",\n",
    "    line_width=2,\n",
    "    annotation_text=f\"-Observed: {-observed_diff:.4f}\",\n",
    "    annotation_position=\"top left\"\n",
    ")\n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "    width=800,\n",
    "    height=500,\n",
    "    showlegend=False\n",
    ")\n",
    "fig.show()\n",
    "fig.write_html('rate-the-review/assets/fairness_test.html', include_plotlyjs='cdn')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary\n",
    "print(f\"Groups Compared:\")\n",
    "print(f\"Quick recipes:  {median_minutes:.1f} minutes ({quick_mask.sum():,} reviews)\")\n",
    "print(f\"Slow recipes: > {median_minutes:.1f} minutes ({slow_mask.sum():,} reviews)\")\n",
    "print(f\"\\nEvaluation Metric: Precision for highly_rated = 1\")\n",
    "print(f\"\\nObserved Performance:\")\n",
    "print(f\"Quick recipes precision: {quick_precision:.4f}\")\n",
    "print(f\"Slow recipes precision: {slow_precision:.4f}\")\n",
    "print(f\"Difference (Quick - Slow): {observed_diff:.4f}\")\n",
    "print(f\"\\nPermutation Test Results:\")\n",
    "print(f\"P-value (two-sided): {p_value:.4f}\")\n",
    "print(f\"Significance level: 0.05\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsc80",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
